---
title: "The Office Report"
author: "Carlos Omar Pardo, Antonio Lopez Ruiz, Jose Lopez Torres"
date: "December 3, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(forcats)
library(gsheet)
library(tidyverse)
library(RColorBrewer)
theoffice_url <- 'https://docs.google.com/spreadsheets/d/18wS5AAwOh8QO95RwHLS95POmSNKA2jjzdt0phrxeAE0/edit#gid=747974534'
theoffice_df <- gsheet2tbl(theoffice_url)
```

# Introduction

## Instructions: Explain why you chose this topic, and the questions you are interested in studying.

This team (just like over 6 million viewers per episode) has enjoyed watching this show, and decided to use our data visualization techniques to ask some complex questions about the series. We were motivated to have a better understanding over the series in a quantitative manner, by creating a framework with three types of questions. We expect to be able to solve them mostly based on the data we obtained from the scripts and other web sources, rather than from our experience as audience.

Can we get a better understanding on each character's specific profile, like who says the most jokes, or who holds the most airtime?

Can this dataset help us understand something about the interaction among the characters, like who mentions whom the most times, or how many scenes are shared by groups like Michael and Dwight, or Jim and Pam?

Finally, would the scripts help us understand any impact the show might have had on society? For instance, do males play a bigger role on the show? How did viewership change as time passed and how did it react to changes in the original cast?

Using these questions as a starting point, we dived into our database and expanded it by using information that we deemed appropriate, but wasn't in it originally. 

## Instructions: List team members and a description of how each contributed to the project This goes last within the section.

As a team, each one of us contributed to every general task (cleaning, analysis, documentation, presentation and interactivity) in order to practice the skills we learnt in class. In broader terms, each one contributed more strongly in the area that suit them best: Omar focused in the data cleaning process, creating functions that would ease this process throughout the project as we found obstacles. Antonio worked in planning the interactive component, thinking what visualizations should be included and how to help the audience understand the information we would eventually give them. Jose worked in the general structure of the document, and assigning the analyses to their appropriate sections. We also made an effort to share our knowledge by solving each other's questions and agreeing on how to advance in the exploration of our dataset, in order to maximize our learning from the assignment.

# Description of Data

## Instructions: Describe how the data was collected, how you accessed it, and any other noteworthy features.
## Citations are needed
Thanks to the show's large number of fans there are several websites dedicated to different aspects of the series. We found a repository from a fan's blog, called Officequotes.net. This repository includes every dialogue from the script, specifying the season, episode and scene in which it was said, as well as its speaker and whether the scene was edited out of the show or not.

The data is stored as a Google Sheet in (https://docs.google.com/spreadsheets/d/18wS5AAwOh8QO95RwHLS95POmSNKA2jjzdt0phrxeAE0/edit#gid=747974), so we reviewed it to understand the conditions it was in. In general, the dataset was properly structured -which eased the cleaning process. The tasks we followed are described in detail on the next session of the report.

Furthermore, one of the questions required us to retrieve data that wasn't available on the original dataset. We found ratings at a per-episode level in wikipedia.org (which took this information from nielsen.com) and copied the information on a csv file. Although the ratings correspond to the original airdate's audience, we assume that this factor is a constant (i.e, it is a fair comparisson to evaluate conclusions about viewership for any episode in the seried based on the rating recieved during its first cast), and used it for our project. We also created a table with the main cast and the gender of each character in order to include this factor in our analysis. The genders were populated manually into the csv file.

An important element in the analysis of the dialogues was the selection of a stopword dictionary, to drop (and analyze) common words in the language, so that we only retained meaningful information in our dataset. We used the (stopwrods) default library for this matter.

# Analysis of data quality

## Instructions: Provide a detailed, well-organized description of data quality, including textual description, graphs, and code.

In order to assess the data quality, we set up a few tasks to inspect different questions about the transcripts: Were conversations missing? Was a certain standard on names followed? Could we really find answers to the questions we were hoping for with the information that was gathered?

First, we inspected the general structure of the original dataframe. The scripts included 7 variables as previously discussed, and we understood it was necessary to use the numbers as categorical data rather than integers (since they represent episodes, scenes, seasons, etc.) and the speaker names as factors, rather than character strings. This idea helped us realize that there were some typos in the names of the characters in the dataframe, which we started fixing.

## Also, a lot of different combinations are missing, like micahel for michael 

```{r glimpse}
glimpse(theoffice_df)
```

Furthermore, a character might have been referenced in different ways throughout the show, like "Todd Packer" for "Todd", or "Robert California" for "Robert". Also, standardizing names in lower case would facilitate the process later, so we also included that adjustment.

```{r fct_character}
#theoffice_df$speaker <- as.factor(theoffice_df$speaker)
#levels(theoffice_df$speaker)
```

Later, we inspected for any N/As in the dataset, using the visna function. We found that there were none of these values in the entire data (nice!), but this did not mean that the data was usable straight away. 

```{r missing, echo=TRUE}
visna(theoffice_df, sort = "b")
```

##Maybe we should drop this from this section.

After these steps, we know that in our analyses we must limit the number of characters used, as our intent is to focus on the main cast, rather than on any guest or extra that might have said a line in the entire series.

```{r cast, echo=TRUE}

main_and_sec_chars <- dialogues_df %>% 
  group_by(speaker) %>% 
  summarise(Freq = n()) %>% 
  arrange(-Freq) %>% 
  top_n(30) %>% 
  .$speaker

```

This was great progress to start our analysis, as we could now focus on the text of the actual dialogues, the main point of this project. We followed the process by removing the wrylies, which allowed us to focus on dialogues rather than instructions. We also dropped punctuation and removed capitalization from the document. Finally, we tied common expressions throughout the series, joining each word by an underscore, in order to avoid dropping the stopwords on these expressions later in the project.

## Note: We need to decide what happens with the stop words. I'm leaving this comment as a placeholder.



## Note: Description about ngrams


The file with the whole set up of data clean-up functions and the process rests in 'file="../Cubicles/Omar/clean_data.R' for your review.

# Main Analysis

## Instructions: Provide a detailed, well-organized description of your findings, including textual description, graphs, and code. Your focus should be on both the results and the process. Include, as reasonable and relevant, approaches that didn’t work, challenges, the data cleaning process, etc.

## Remember to add charts and conclusions. Use a color deficiency compliant theme like "scale_fill_brewer(palette="Set1")"

Now that the data had been cleaned, we can move on to figuring out what relevant information can be obtained about the characters, their interactions, and get a better understanding of the series. We are following the framework we devised at the beginning of the project. 

## What can we learn about individual characters?

Upon organizing the dataset, we realized that one of the most obvious quesitons was if it was possible to determine what characters have the most lines in the show.

```{r most_dialogues, echo=TRUE}

dialogues_by_char <- dialogues_df %>% 
  group_by(speaker) %>% 
  summarise(Freq = n()/1000) %>% 
  arrange(Freq) %>%
  top_n(15)

ggplot(dialogues_by_char, aes(x = reorder(speaker, -Freq), y = Freq)) + 
  geom_col() + 
  labs(title = "Michael, Dwight and Jim speak the most lines in the series", x = "Character", y="Number of lines (in thousands)") + 
  scale_color_brewer(palette="Set1")

```

It was obvious we could solve this question, but we faced an issue when we realized there are hundreds of characters with a single line. Since we only care about the main characters, we filtered the top 15 speakers of the entire series, and this was easier to plot.

But does this mean these characters really speak the most? We want to eliminate the possibility that a speaker says several very short lines, which might make them get into the main cast without actually developing a character in the show.

```{r most_words, echo=TRUE}
words_by_char <- words_df %>% 
  group_by(speaker) %>% 
  summarise(Freq = n()/1000) %>% 
  arrange(-Freq) %>%
  top_n(15)

ggplot(words_by_char, aes(x = reorder(speaker, -Freq), y = Freq)) + 
  geom_col() + 
  labs(title = "Michael, Dwight and Jim also speak the most words in the show", x = "Character", y="Number of words (in thousands)") + 
  scale_color_brewer(palette="Set1")

```

This chart allows us to understand 2 things: there is a large gap after the top 5 characters in both number of lines and in words per character, and the general participation of the cast doesn't change. This might be due to the number of seasons and episodes, where everyone had a proportional chance to say a number of lines with a proportional number of words. We noticed it is difficult to understand this as 2 separate charts, so we will merge them into a single one in the executive summary.

The next step to determine if the distribution of screen time is proportional indeed, we would like to find out if there is a relation between the average characters per line of speech for each character, and the maximum length of a line they delivered in the show. 

```{r longest_lines, echo=TRUE}

dialogues_df <- wrylies_dialogue_df %>% 
  filter(!deleted) %>% 
  select(id, season, episode, scene, speaker, dialogue) %>% 
  mutate(filt_dialogue = clean_text(dialogue, ct_list), dialogue_length = nchar(filt_dialogue))

dialogue_length_df <- dialogues_df %>%
  select (speaker, dialogue_length) %>%
  group_by(speaker) %>% 
  summarize (avg_d_length = sum(dialogue_length)/n(), max_dialogue = max(dialogue_length)) %>%
  top_n(15)

filtered_dial <- gather(dialogue_length_df, attribute, value, -speaker)

ggplot(filtered_dial, aes(x = value, y = fct_reorder2(speaker, attribute == 'max_dialogue', value, .desc = FALSE))) +
  geom_point(aes(color = attribute)) + 
  labs(title = "Longest line and average length of line for each character", x = "Number of characters", y="Character")+ 
  theme(legend.position="top", legend.title=element_blank() ) + scale_colour_discrete(labels=c("Average dialogue", "Longest dialogue"))

```

From this chart, we can see that the lines of each character in the main cast are of the same length, approximately (although it would be interesting to take a closer look into this matter). We can also determine that the 5 main characters of the show are amongst the 9 characters with the longest line said, which makes sense due to how their characters evolved throughout the show. Particularly, Andy's longest line in the show is the one that ranks the lowest, with Robert, Jan, Erin and DeAngelo topping him for longest dialogue in the show.

Now that we have our hands in this data, we realized that we can go one level deeper, and replicate at a word level what we just did for dialogues. This might shine some light over a character being portrayed as smarter in the show, as the ability to use more complicated words is linked to the perception of intelligence. 


3) Who is the character that says the longest words?
```{r longest, echo=TRUE}
### Changed words_df; leaving it here in the meanwhile
words_df <- dialogues_df %>% 
  separate_rows(., filt_dialogue, sep = " ", convert = TRUE) %>% 
  rename(word = filt_dialogue) %>% 
  filter(word != "") %>%
  mutate(word_length = nchar(word))

word_length_df <- words_df %>%
  select (speaker, word_length) %>%
  group_by(speaker) %>% 
  summarize (avg_length = sum(word_length)/n(), max_word = max(word_length)) %>%
  top_n(15)

filtered_word <- gather(word_length_df, attribute, value, -speaker)

ggplot(filtered_word, aes(x = value, y = fct_reorder2(speaker, attribute == 'max_word', value, .desc = FALSE))) +
  geom_point(aes(color = attribute)) + 
  labs(title = "Longest word and average length of word for each character", x = "Number of characters", y="Character") + 
  theme(legend.position="top", legend.title=element_blank() ) + scale_colour_discrete(labels=c("Average word", "Longest word"))

```





## Note: We discussed about making it interactive, as the % is split on the previous (3) questions

5) Who uses the most crutch words?
Antonio

6) Who got cut out? Who lost the most scenes due to the editors?
Antonio

8) Who stares at the camera the most?
Antonio

## What can we know about groups of characters and about their interactions?

2) Who are the characters that are mentioned the most in others' dialogues?
```{r interactions, echo=TRUE}
## Too long, might be necessary to move elsewhere; just copied and pasted from Omar's
replace_characters <- function(text) {
  text <- gsub("michael_scott", "michael", text)
  text <- gsub("dwight_schrute", "dwight", text)
  text <- gsub("jim_halpert", "jim", text)
  text <- gsub("andy_bernard", "andy", text)
  text <- gsub("david_wallace", "david", text)
  text <- gsub("robert_california", "robert", text)
  text <- gsub("ryan_howard", "ryan", text)
  text <- gsub("todd_packer", "todd", text)
  text <- gsub("stanley_hudson", "dwight", text)
  text <- gsub("bob_vance", "bob", text)
  text <- gsub("mr_scott", "michael", text)
  return(text)
}

main_and_sec_chars <- dialogues_df %>% 
  group_by(speaker) %>% 
  summarise(Freq = n()) %>% 
  arrange(-Freq) %>% 
  top_n(30) %>% 
  .$speaker

interact_df <- words_df %>% 
  mutate(word = replace_characters(word)) %>% 
  filter(word %in% main_and_sec_chars)

get_interactions_by_char <- function(char, interact_df) {
  interact_char <- interact_df %>% 
    filter(speaker == char)
  interactions <- table(interact_char$word) %>% 
    as.data.frame() %>% 
    arrange(-Freq)
  return(interactions)
}

get_interactions_by_char("kelly", interact_df)

```

7) What are the groups of characters that share the most screen time?
```{r chars_scene, echo=TRUE}
##Shall we move the function elsewhere?
get_chars_involved <- function(speaker) {
  speaker %>% 
    unique %>% 
    sort %>% 
    paste_words("_")
}

chars_per_scene <- words_df %>% 
  group_by(season, episode, scene) %>% 
  summarise(chars = get_chars_involved(speaker)) %>% 
  ungroup %>% 
  group_by(chars) %>% 
  summarise(Freq = n()) %>% 
  arrange(-Freq) 

```

## The Office in real life: portrayal of Corporate America and viewership trends through time

1) How many "that's what she said" gags took place throughout the series?


```{r TWSS, echo=TRUE}
thats_what_she_said <- words_df %>% 
  filter(word == "thats_what_she_said") %>% 
  group_by(speaker) %>% 
  summarise(Freq = n()) %>% 
  arrange(-Freq)

ggplot(thats_what_she_said, aes(x=reorder(speaker, -Freq), y=Freq)) +
  geom_col() + 
  labs(title = "That", x = "Character", y="Total number of gags") + 
  scale_fill_brewer(palette="Set1")

```


9) Do female characters have as much dialogues as male characters? 


```{r female_male, echo=TRUE}
gen <- read.csv(file="../Cubicles/Jose/main_cast_gender.csv", header=TRUE, sep=",")

ggplot(gen, aes(x=fct_infreq(gen$gender))) + 
  geom_bar(aes(y=(..count..))) + 
  labs(title = "Over 2/3 of the show's main cast are male", x = "Gender", y="Number of characters")

## Merge for number of dialogues
gen_dialogues <- merge(dialogues_by_char, gen, all.x = TRUE) %>%
  filter(gender %in% c('male', 'female')) %>%
  group_by(gender) %>% 
  summarize(Freq = sum(Freq)/1000) %>% 
  arrange(-Freq)

ggplot(gen_dialogues, aes(x = reorder(gender, -Freq), y = Freq)) + 
  geom_col() + 
  labs(title = "Do females have more dialogues?", x = "Gender", y="Number of lines (in thousands)") + 
  scale_color_brewer(palette="Set1")
  
## Merge for number of words
gen_words <- merge(words_by_char, gen, all.x = TRUE) %>%
  filter(gender %in% c('male', 'female')) %>%
  group_by(gender) %>% 
  summarize(Freq = sum(Freq)/1000) %>% 
  arrange(-Freq)

ggplot(gen_words, aes(x = reorder(gender, -Freq), y = Freq)) + 
  geom_col() + 
  labs(title = "Is the amount of words proportional to the number of dialogues by gender?", x = "Gender", y="Number of words (in thousands)") + 
  scale_color_brewer(palette="Set1")

```

10) As the show progressed, how did ratings change?
Antonio


## Questions that might have a shorter answer or depend on the other questions we solved

13) Is Michael really smarter than Oscar?

14) Can we visualize how characters interact amongst themselves?
Antonio?

15) Can we identify the most important word for a given character or episode?


# Executive Summary

## Req. Provide a short nontechnical summary of the most revealing findings of your analysis written for a nontechnical audience. The length should be approximately two pages (if we were using pages…) Take extra care to clean up your graphs, ensuring that best practices for presentation are followed.

## Req. cont. Note: “Presentation” here refers to the style of graph, that is, graphs that are cleaned up for presentation, as opposed to the rough ones we often use for exploratory data analysis. You do not have to present your work to the class! However, you may choose to present your work as your community contribution, in which case you need to email me to set a date before the community contribution due date. (The presentation itself may be later.)



# Interactive component

## Instructions: Select one (or more) of your key findings to present in an interactive format. Be selective in the choices that you present to the user; the idea is that in 5-10 minutes, users should have a good sense of the question(s) that you are interested in and the trends you’ve identified in the data. In other words, they should understand the value of the analysis, be it business value, scientific value, general knowledge, etc.



# Conclusion

## Instructions: Discuss limitations and future directions, lessons learned.




